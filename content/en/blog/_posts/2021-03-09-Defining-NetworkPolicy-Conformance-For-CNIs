---
layout: blog
title: "Defining NetworkPolicy Conformance for CNI providers"
date: 2021-03-21
slug: defining network policy conformance
---

Authors: Matt Fennwick, Jay Vyas, Ricardo Katz, Amim Knabben, Douglas Landgraf

A brief conversation around "node local" network policies in April of 2020 inspired the creation of a NetworkPolicy subproject from sig-network.

In this post we'll discuss:

- Why we created a Subproject for Network Policies 
- How we changed the Kubernetes e2e framework to `visualize` NetworkPolicy implementation of your CNI provider
- The initial results of our comprehensive NetworkPolicy Conformance validator, `Cyclonus`, built around these principles
- Improvements we've made to the NetworkPolicy user experience

## Why we created a subproject for NetworkPolicies

In April of 2020 it was becoming clear that many CNIs were emerging, and many vendors implement these CNIs in different ways.  Users were beggining to express a little bit of confusion around how to implement policies for different scenarios, and asking for new features.
It was clear that we needed to begin unifying the way we think about network policies in Kubernetes, if we wanted to avoid becoming an "ancient" API which was going to be overrun by implementation divergences. 

For example:  
- Calico as a CNI provider can be run using IPIP or VxLan mode.  CNI's such as Antrea and Cillium offer similar configuration divergences as well.
- Some CNIs rely on IPTables for NetworkPolicies (i.e. calico), whereas others use a completely different technology stack (for example, the antrea project uses open vswith rules).
- Some CNI providers only implement a subset of the Kubernete NetworkPolicy API (for example, certain providers don't support the ability to target a `NamedPort`, or a port range, which is a newer feature in the K8s API).

Thus, end-users need to follow a multistep process to implement network policies to secure their applications:
- Confirm that their CNI supports NetworkPolicys (some don't, such as flannel)
- Confirm that their CNI supports the specific NetworkPolicy's that a user is interested in (again, the Named port or Port range examples come to mind here)
- Confirm that their application's network policy definitions are doing the 'right thing'
- Find out the vendor specific implementation of a policy, and check wether or not that implementation had a CNI neutral implementation (which is preferable for most users)

The NetworkPolicy project in upstream Kubernetes aims at providing a community where people can learn about, and contribute to, the Kubernetes NetworkPolicy API and the surrounding ecosystem.

## The First step: A validation framework for NetworkPolicies that was intuitive to use and understand

The Kubernetes end to end suite has always had NetworkPolicy tests, but these weren't run in CI, and the way they were implemented didn't really provide any holistic, easily consumable information about how a policy was working in a cluster.
This is because the original tests didn't provide any kind of visual summary of connectivity across a cluster.   We thus initially set out to make it easy to confirm CNI support for NetworkPolicies by
making the end to end tests (which are often used by administrators or users to diagnose cluster conformance) easy to interpret.

To solve the problem of confirming that CNI's support the basic features one cares about for a policy, we built a new NetworkPolicy validation tool into the Kubernete e2e framework which allows for visual inspection of policys and their effect on a standard set of pods in a cluster.
For example, in the following test output.  As an example, we found a bug in [ovn kubernetes](https://github.com/ovn-org/ovn-kubernetes/issues/1782) with this tool that was really easy to characterize,
wherein certain policies caused a state-modification that, later on, resulted in traffic being unnecesarily blocked (even after all network policies, cluster wide, were deleted).

```
Nov  3 15:58:54.034: INFO: Network Policy creating netpol-3291-x/allow-ingress-port-80                                                                                                                                            
metadata:                                                                                                                                                                                                                         
  creationTimestamp: null                                                                                                                                                                                                         
  name: allow-ingress-port-80                                                                                                                                                                                                     
spec:                                                                                                                                                                                                                             
  ingress:                                                                                                                                                                                                                        
  - ports:                                                                                                                                                                                                                        
    - port: serve-80-tcp                                                                                                                                                                                                          
  podSelector: {} 
...

Nov  4 16:58:43.449: INFO: expected:

-               netpol-5599-x/a netpol-5599-x/b netpol-5599-x/c netpol-5599-y/a netpol-5599-y/b netpol-5599-y/c netpol-5599-z/a netpol-5599-z/b netpol-5599-z/c
netpol-5599-x/a .               .               .               .               .               .               .               .               .
netpol-5599-x/b .               .               .               .               .               .               .               .               .
netpol-5599-x/c .               .               .               .               .               .               .               .               .
netpol-5599-y/a .               .               .               .               .               .               .               .               .
netpol-5599-y/b .               .               .               .               .               .               .               .               .
netpol-5599-y/c .               .               .               .               .               .               .               .               .
netpol-5599-z/a .               .               .               .               .               .               .               .               .
netpol-5599-z/b .               .               .               .               .               .               .               .               .
netpol-5599-z/c .               .               .               .               .               .               .               .               .
.......... Nov  4 16:58:43.449: INFO: observed:

-               netpol-5599-x/a netpol-5599-x/b netpol-5599-x/c netpol-5599-y/a netpol-5599-y/b netpol-5599-y/c netpol-5599-z/a netpol-5599-z/b netpol-5599-z/c
netpol-5599-x/a X               X               X               X               X               X               X               X               X
netpol-5599-x/b X               X               X               X               X               X               X               X               X
netpol-5599-x/c X               X               X               X               X               X               X               X               X
netpol-5599-y/a .               .               .               .               .               .               .               .               .
netpol-5599-y/b .               .               .               .               .               .               .               .               .
netpol-5599-y/c .               .               .               .               .               .               .               .               .
netpol-5599-z/a .               .               .               .               .               .               .               .               .
netpol-5599-z/b .               .               .               .               .               .               .               .               .
netpol-5599-z/c .               .               .               .               .               .               .               .               .

```

This was one of our earliest 'wins' in the NetworkPolicy group, as we were able to identity and work with the OVN kubernetes group to fix a bug in egress policy processing.

However, even though this tool has made it easy to validate roughly 30 common scenarios, it doesn't validate *all* network policy scenarios - because there are an enormous permutation of possible
policies that one might create (well, technically, we might say this number is infinite given that theres an infinite number of possible namespace/pod/port/prototocol variations one can create).

Once these tests were in play, we worked with the Upstream sig-network and sig-testing communities (thanks to Antonio Ojea and Ben Elder) to put a tesgrid network policy job in place.  THis job
continuously runs the entire suite of sig-network NetworkPolicy tests against GCE with Calico as a networkpolicy provider (https://testgrid.k8s.io/sig-network-gce#presubmit-network-policies,%20google-gce).

Part of our role as a subproject is to help make sure that, when these tests break, we can help triage them effectively.

## Cylconus: The next step towards NetworkPolicy Confirmance

Around the time that we were finishing the validation work, it became clear from the community that, in general, we needed to solve the overall problem of testing ALL possible NetworkPolicy implemetnations.
For example, a KEP was recently written which introduced the concept of micro versioning to Network Policies to accomodate describing this at the API level (https://github.com/kubernetes/enhancements/pull/2137/files), by Dan Winship.  

In response to this increasingly obvious need to comprehensively defined network policy for all vendors, Matt Fennwick decided to evolve our approach to NetworkPolicy validation again by creating Cyclonus.

Cyclonus is a comprehensive network policy fuzzing tool which verifies a CNI provider against 100s of different NetworkPolicy scenarios, by defining the same truth table/policy combinations as done in the end to end tests, while Also
providing a hiherachihcal representation of policy "categories".  We've found some interesting missing implementations in almost every CNI we've tested, so far, and have even contributed some fixes back, for example:

To run a cyclonus job, you create a YAML file like so:

```
apiVersion: batch/v1
kind: Job
metadata:
  name: cyclonus
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
        - command:
            - ./cyclonus
            - generate
            - --perturbation-wait-seconds=15
            - --server-protocol=tcp,udp
          name: cyclonus
          imagePullPolicy: IfNotPresent
          image: mfenwick100/cyclonus:latest
      serviceAccount: cyclonus
```

From there, cyclonus outputs a printout of all test cases it will run:
```
test cases to run by tag:
- target: 6
- peer-ipblock: 4
- udp: 16
- delete-pod: 1
- conflict: 16
- multi-port/protocol: 14
- ingress: 51
- all-pods: 14
- egress: 51
- all-namespaces: 10
- sctp: 10
- port: 56
- miscellaneous: 22
- direction: 100
- multi-peer: 0
- any-port-protocol: 2
- set-namespace-labels: 1
- upstream-e2e: 0
- allow-all: 6
- namespaces-by-label: 6
- deny-all: 10
- pathological: 6
- action: 6
- rule: 30
- policy-namespace: 4
- example: 0
- tcp: 16
- target-namespace: 3
- named-port: 24
- update-policy: 1
- any-peer: 2
- target-pod-selector: 3
- IP-block-with-except: 2
- pods-by-label: 6
- numbered-port: 28
- protocol: 42
- peer-pods: 20
- create-policy: 2
time="2021-03-11T14:56:50Z" level=info msg="test #2 to run: set namespace to y"
- policy-stack: 0
- any-port: 14
- delete-namespace: 1
- delete-policy: 1
- create-pod: 1
- IP-block-no-except: 2
- create-namespace: 1
- set-pod-labels: 1
testing 112 cases
```
Note that, cyclonus 'tags' its tests based on the type of policy being created, because of the fact that the policies themselves are auto-generated, and thus have no meaningful names to be recognized by.

For each test, cyclonus outputs a truth table, which is again similar to that of the E2E tests, along with the policy being validated:

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: null
  name: base
  namespace: x
spec:
  egress:
  - ports:
    - port: 81
    to:
    - namespaceSelector:
        matchExpressions:
        - key: ns
          operator: In
          values:
          - "y"
          - z
      podSelector:
        matchExpressions:
        - key: pod
          operator: In
          values:
          - a
          - b
  - ports:
    - port: 53
      protocol: UDP
  ingress:
  - from:
    - namespaceSelector:
        matchExpressions:
        - key: ns
          operator: In
          values:
          - x
          - "y"
      podSelector:
        matchExpressions:
        - key: pod
          operator: In
          values:
          - b
          - c
    ports:
    - port: 80
      protocol: TCP
  podSelector:
    matchLabels:
      pod: a
  policyTypes:
  - Ingress
  - Egress

0 wrong, 0 ignored, 81 correct
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| TCP/80 | X/A | X/B | X/C | Y/A | Y/B | Y/C | Z/A | Z/B | Z/C |
| TCP/81 |     |     |     |     |     |     |     |     |     |
| UDP/80 |     |     |     |     |     |     |     |     |     |
| UDP/81 |     |     |     |     |     |     |     |     |     |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/a    | X   | X   | X   | X   | X   | X   | X   | X   | X   |
|        | X   | X   | X   | .   | .   | X   | .   | .   | X   |
|        | X   | X   | X   | X   | X   | X   | X   | X   | X   |
|        | X   | X   | X   | X   | X   | X   | X   | X   | X   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/b    | .   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/c    | .   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/a    | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/b    | .   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/c    | .   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/a    | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/b    | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/c    | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
```

Both cyclonus and the e2e tests use the same strategy to validate a network policy of "probing" pods over TCP or UDP.  
As an example of how we use Cyclonus to help make CNI implementations better from a NetworkPolicy perspective, you can see 

- https://github.com/cilium/cilium/issues/14678 an issue we've found in cillium related to default port numbers 
- https://github.com/projectcalico/libcalico-go/pull/1373 an issue weve found and fixed in calico related to default TCP protocols
- https://github.com/vmware-tanzu/antrea/issues/1764 an issue we've found in Antrea, related to CIDR ingress 

The good news is, all of these issues are actively being fixed and iterated on between sig-network, the cni providers, and the network policy subproject.  

If you're interested in verifying NetworkPolicy functionality on your cluster (and, if you care about security, or run a SaaS, you should be), then you can run the upstrea end to end tests, or cyclonus, or both.
- If you are deeply curious about your CNI providers network policy implementation, use cyclonus to test *100s* of policies, and evaluate your CNI for comprehensive functionality, for deep discovery of potential security holes, or for getting involved with the upstream networkpolicy efforts.
- If your just getting started with network policies, your better off running the e2e tests if you want to simply verify the "common" network policy cases that most CNIs should be implementing correctly, in a way that is quick to diagnose.

## Where to start with NetworkPolicy testing ?

- You can run cyclonus on your cluster easily by following the instructions at https://github.com/mattfenwick/cyclonus, and determine wether *your* specific CNI configuration is
fully conformant to the 100s of different Kubernetes Network Policy API constructs.
- Alternatively, you can use a tool like `sonobuoy` (https://github.com/vmware-tanzu/sonobuoy) to run the existing E2E tests in Kubernetes, with the `--ginkgo.focus=NetworkPolicy` flag.  Note that only versions of sonobuoy built for k8s 1.21 and above will have the *new* network policy tests in them.

## Improvements to the NetworkPolicy API and USer Experience

In addition to cleaning up the validation story for CNI providers over NetworkPolicies, we've also spent some time improving the K8s API for a few commonly requested NetworkPolicy features.
After months of delibration, we eventually settled on a few core areas for improvement:

- Port Range policies: We now allow you to specify a *range* of ports for a policy.  This allows users interested in scenrios like FTP or virtualization to enable advanced policies.  The port range
option for networkpolicies will be available to use in Kubernetes 1.21.  You can read more about this here https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/2079-network-policy-port-range. 
- Namespace as name policies: Allowing users in Kubernetes 1.21 and forward to target namespaces by default labels that are always on all namespaces.  This was done in collaboration with jordan ligget and tim hockins on the apimachinery side.
This change allowed us to improve the network policy user experience without actually changing the API ! For details, you can read about it here https://github.com/kubernetes/enhancements/pull/2162. 
The TLDR is that after Kubernetes 1.21, ALL NAMESPACES will have the following label, addd by default: 

```
kubernetes.io/metadata.name: my-namespace
```

This means, you can write an namespace policy against this namespace, even if you can't edit its labels.  For example, this policy, will 'just work', without needing to
run a command such as `kubectl edit namespace`.  In fact, it will even work if you can't edit or view this namespace's data at all, because of the magic of apiserver defaulting.

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: my-namespace
```

## The Future 

We're also working on some improvements for the future of network policies, including:

- Fully qualified Domain policies: The google cloud team created a prototype (which we are really excited about) of FQDN policies, https://github.com/GoogleCloudPlatform/gke-fqdnnetworkpolicies-golang.  This tool uses the network policy API to enforce policies
against L7 URLs, by finding their IPs and blocking them proactively when requests are made.
- Cluster Administrative policies: We're working hard at enabling *administrative* or *cluster scoped* network policies for the future. These are being presented iteratively to the NetworkPolicy subproject.
You can read about them here https://docs.google.com/presentation/d/1Jk86jtS3TcGAugVSM_I4Yds5ukXFJ4F1ZCvxN5v2BaY/edit?usp=sharing.

The Network Policy subproject meets on mondays at 4PM EST. For details, check out the sig-network community repo https://github.com/kubernetes/community/tree/master/sig-network.  We'd love
to hang out with you, hack on stuff, and help you adopt K8s network policies for your cluster wherever possible.

### A quick note on User Feedback

We've gotten alot of ideas and feedback from users on Network Policies.  Feedback is a double-edged sword.  Alot of people have interesting ideas about network policies, but we've found that as a subproject, very few people were deeply interested in implementing these ideas to the full extent.
Almost every change to the NetworkPolicy API includes weeks or months of discussion to cover different cases, and ensure no CVEs are being introduced.  Thus, long term ownership is the biggest impediment in improving the NetworkPolicy
user experience for us, over time.
- We've documented alot of the  history of the Network Policy dialogue here (https://github.com/jayunit100/network-policy-subproject/blob/master/history.md).
- We've also taken a poll of users, for what they'd like to see in the network policy API here (https://github.com/jayunit100/network-policy-subproject/blob/master/history.md)

We encourage anyone to provide us with feedback, but our most pressing issues right now involve finding long term owners to help us drive changes.  This doesn't require alot of technical
knowledge, but rather, just a long term commitment to helping us stay organized, do paperwork, and iterate through the many stages of the K8s feature process.  If you want to help us
and get involved, please reach out on the sig-network mailing list, or in the sig-network room in the k8s.io slack channel !


